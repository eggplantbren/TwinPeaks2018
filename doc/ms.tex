%  LaTeX support: latex@mdpi.com 
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

% You need to save the "mdpi.cls" and "mdpi.bst" files into the same folder as this template file.

%=================================================================
\documentclass[entropy,article,submit,moreauthors,pdftex,10pt,a4paper]{Definitions/mdpi} 

\usepackage{algorithm2e}
\usepackage{dsfont}
\usepackage{setspace}

% If you would like to post an early version of this manuscript as a preprint, you may use preprint as the journal and change 'submit' to 'accept'. The document class line would be, e.g. \documentclass[preprints,article,accept,moreauthors,pdftex,10pt,a4paper]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%
%--------------------
% Class Options:
%--------------------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, aerospace, agriculture, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, applsci, arts, asi, atmosphere, atoms, axioms, batteries, bdcc, behavsci, beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsci, buildings, carbon, cancers, catalysts, cells, ceramics, challenges, chemengineering, chemosensors, children, cleantechnol, climate, clockssleep, cmd, coatings, colloids, computation, computers, condensedmatter, cosmetics, cryptography, crystals, cybersecurity, data, dentistry, designs, diagnostics, dairy, diseases, diversity, drones, econometrics, economies, education, electrochem, electrochemistry, electronics, energies, entropy, environments, epigenomes, est, fermentation, fibers, fire, fishes, fluids, foods, forecasting, forests, fractalfract, futureinternet, galaxies, games, gastrointestdisord, gels, genealogy, genes, geohazards, geosciences, geriatrics, hazardousmatters, healthcare, heritage, highthroughput, horticulturae, humanities, hydrology, informatics, information, infrastructures, inorganics, insects, instruments, ijerph, ijfs, ijms, ijgi, ijtpp, inventions, j, jcdd, jcm, jcs, jdb, jfb, jfmk, jimaging, jof, jintelligence, jlpea, jmmp, jmse, jpm, jrfm, jsan, land, languages, laws, life, literature, logistics, lubricants, machines, magnetochemistry, make, marinedrugs, materials, mathematics, mca, medsci, medicina, medicines, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, modelling, molbank, molecules, mps, mti, nanomaterials, ncrna, neonatalscreening, neuroglia, nitrogen, nutrients, ohbm, particles, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, plants, plasma, polymers, polysaccharides, proceedings, processes, proteomes, publications, quaternary, qubs, reactions, recycling, religions, remotesensing, reports, resources, risks, robotics, safety, sci, scipharm, sensors, separations, sexes, sinusitis, smartcities, socsci, societies, soilsystems, sports, standards, stats, surfaces, surgeries, sustainability, symmetry, systems, technologies, toxics, toxins, tropicalmed, universe, urbansci, vaccines, vehicles, vetsci, vibration, viruses, vision, water, wem, wevj
%---------
% article
%---------
% The default type of manuscript is article, but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefreport, casereport, changes, comment, commentary, communication, conceptpaper, correction, conferenceproceedings, conferencereport, expressionofconcern, meetingreport, creative, datadescriptor, discussion, editorial, essay, erratum, hypothesis, interestingimages, letter, meetingreport, newbookreceived, opinion, obituary, projectreport, reply, reprint, retraction, review, perspective, protocol, shortnote, supfile, technicalnote, viewpoint
% supfile = supplementary materials
% protocol: If you are preparing a "Protocol" paper, please refer to http://www.mdpi.com/journal/mps/instructions for details on its expected structure and content.
%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g. the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.
%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{1}
\pubyear{2018}
\copyrightyear{2018}
\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}

%\updates{yes} % If there is an update available, un-comment this line
 
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Nested Sampling with Multiple Objective Functions}

% Author Orchid ID: enter ID or remove command
\newcommand{\xx}{\boldsymbol{\theta}}
\newcommand{\dx}{d\boldsymbol{\theta}}
\newcommand{\data}{D}
\newcommand{\II}{I}
\newcommand{\mnras}{Monthly Notices of the Royal Astronomical Society}
\newcommand{\todo}{\color{blue}{\bf TODO}:~}

%\newcommand{\orcidauthorA}{0000-0000-000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Brendon J. Brewer$^{1,}$*,
        Robert J. N. Baldock$^2$ and
        Ewan Cameron$^3$}

% Authors, for metadata in PDF
\AuthorNames{Brendon J. Brewer, Robert J. N. Baldock and Ewan Cameron}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Statistics, The University of Auckland, Private Bag 92019,
Auckland 1142, New Zealand \\
$^{2}$ \quad TBD \\
$^{3}$ \quad TBD }

% Contact information of the corresponding author
\corres{Correspondence: bj.brewer@auckland.ac.nz}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

% Simple summary
%\simplesumm{}

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{We present a generalization of Skilling's Nested Sampling algorithm
that estimates
the partition function $Z(\beta_1, \beta_2)$ for a family of
probability distributions indexed by two hyperparameters, $\beta_1$ and
$\beta_2$. Problems like this arise frequently in statistical mechanics and
occasionally in Bayesian inference.
The algorithm is based on an interpretation of Nested Sampling in terms of
Sequential Monte Carlo, which produces unbiased estimates of $Z$ and therefore
allows parallel runs to be trivially combined.
We demonstrate the algorithm on three example problems:
(i)
a toy problem whose structure allows us
to accurately calculate $Z(\beta_1, \beta_2)$ numerically using non-Monte-Carlo
methods, so we can compute the actual error in the reconstructed function
$Z(\beta_1, \beta_2)$; (ii) a collection of atoms with a potential
that is intermediate between a Lennard-Jones form and a `polymer' form; and
(iii) {\todo decide between a few options: image reconstruction with an
intractable normalizing constant in the prior, or perhaps the `Potts' example
which is a compromize between a 2D potts model and a product of many 1D
Potts models.}
%(iii) A Bayesian image reconstruction example where the prior for the image
%is defined conditionally on a hyperparameter, but where this prior contains
%an intractable normalizing constant.
A free software implementation in C++ and Python is available at
\url{https://github.com/eggplantbren/TwinPeaks2018}.}

% Keywords
\keyword{nested sampling; bayesian computation; statistical mechanics;
         partition functions;
         sequential monte carlo; multi-objective optimization}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only for the journal Gels: Please place the Experimental Section after the Conclusions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}
%The template details the sections that can be used in a manuscript. Note that the order and names of article sections may differ from the requirements of the journal (e.g. the positioning of the Materials and Methods section). Please check the instructions for authors page of the journal to verify the correct order and names. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX related questions please contact Janine Daum at latex-support@mdpi.com.
%The order of the section titles is: Introduction, Materials and Methods, Results, Discussion, Conclusions for these journals: aerospace,algorithms,antibodies,antioxidants,atmosphere,axioms,biomedicines,carbon,crystals,designs,diagnostics,environments,fermentation,fluids,forests,fractalfract,informatics,information,inventions,jfmk,jrfm,lubricants,neonatalscreening,neuroglia,particles,pharmaceutics,polymers,processes,technologies,viruses,vision

\setlength{\parskip}{0.6em}

\section{Nested Sampling}

Nested Sampling (NS) \citep{skilling2006nested} is general
Monte Carlo algorithm that can solve a wide range of problems in Bayesian
computation, statistical mechanics, and information theory
\citep[e.g.][]{partay2010efficient, exoplanet, baldock2016determining,
brewer2017computing}.
Its key strength its ability to cope with phase transitions and other
features which cause problems for many other methods such as those based
on `annealing' \citep{neal2001annealed}. These methods all fall into that half
of the Monte Carlo landscape that addresses {\em what distribution to sample}
rather than how to sample it.

In a Bayesian inference problem with unknown parameters denoted collectively
by a vector $\xx$, the
posterior distribution for the parameters given a data proposition $\data$ and
a `prior information' proposition $\II$ is:
\begin{eqnarray}
p(\xx | \data, \II) &=&
\frac{p(\xx | \II)p(\data | \xx, \II)}{p(\data | \II)}\\
&=& \frac{\pi(\xx)L(\xx)}{Z}
\end{eqnarray}
where $\pi(\xx)$ is the prior distribution, $L(\xx)$ is the likelihood
function, and $Z$ is the normalising constant, known as the
`marginal likelihood' or `evidence':
\begin{eqnarray}
Z &=& \int \pi(\xx) L(\xx) \, \dx.\label{eqn:evidence}
\end{eqnarray}

$Z$ is useful because it is the prior probability of the data conditional on
the hypothesis that $\xx$ is {\em one of}
the possibilities in the space being integrated over, along with
whatever additional assumptions were made. $Z$ thus plays the role
of a likelihood for the parameter space as a whole, and is a key input in
calculating the posterior probabilities of competing `models' (mutually
exclusive choices of $\II$).
Naively, Equation~\ref{eqn:evidence} appears straightforward to compute because
it is a simple expectation of $L$ with respect to the distribution $\pi$.
However,
NS is required because Equation~\ref{eqn:evidence}
is the expected value of $L$ with respect to a very heavy-tailed distribution
(the distribution of $L$-values implied by $\pi(\xx)$). This means a very
large number of samples would be required.
Hence, there is a strong
connection between NS and ideas from rare event simulation
(\citet{walter2017point} has explicitly explored this connection).
Another way of thinking about this
challenge is that the $Z$ integral is dominated by a region with very
high values of $L$ but very low probability according to $\pi$ (and usually
very low volume according to $d\xx$, as well).

The main goal of NS is to compute $Z$, but it can also be used to make
Monte Carlo approximations of the posterior distribution, and to
calculate the `information',
or Kullback-Leibler divergence from the prior to the posterior:
\begin{eqnarray}
H &=& \int p(\xx | \data, \II) \log
\left(\frac{p(\xx | \data, \II)}{p(\xx | \II)}\right) \, d\xx \\
&=& \int \frac{\pi(\xx) L(\xx)}{Z} \log
\left(\frac{L(\xx)}{Z}\right) \, d\xx.
\end{eqnarray}
$H$ quantifies how compressed the posterior distribution is with
respect to the prior. For example, $H = 100$ nats (the units obtained when
the logarithm is the natural logarithm) implies, loosely speaking,
that the posterior is to be found in a tiny region of the space that
has a prior probability of only $e^{-100}$.
$H$ can be interpreted quite literally as a measure of how much
the specific data $\data$ resolved the question ``what is the value
of $\xx$, precisely?'' \citep{knuth_questions, van2017inquiry}.
\citet{brewer2017computing} introduced a version of NS that computes
Shannon entropies, including entropies of marginal distributions (whereas
standard NS can only compute divergences for full joint distributions).

NS works by drawing particles from the
prior $\pi(\xx)$ and successively imposing constraints on the value of
the likelihood $L(\xx)$ that compress the prior mass by a (roughly) known
and constant factor. Like related algorithms such as those
based on `annealing' or `tempering', NS
moves through a sequence of probability distributions, beginning with the
prior. It is the specific sequence of distributions used that distinguishes
NS from annealing. The sequence of distributions used in NS is defined by
\begin{eqnarray}
p(\xx; \ell) &=& \frac{1}{X(\ell)}\pi(\xx)\mathds{1}\left(L(\xx) > \ell\right).
\label{eq:constrained_prior}
\end{eqnarray}
where $X(\ell) = \int \pi(\xx)\mathds{1}\left(L(\xx) > \ell\right)$ is the
normalising constant of the distribution constrained by the condition
$L(\xx) > \ell)$. $X$ is also the quantile function of the likelihoods.

The marginal likelihood can be computed by numerically integrating the
function $L(X)$:
\begin{eqnarray}
Z &=& \int_0^1 L(X)\, dX.
\end{eqnarray}
\citet{salomone2018unbiased}
recently showed how NS can be viewed as an instance of
Sequential Monte Carlo (SMC), with a particular choice of the sequence of
distributions. By weighting the sequence of discarded points differently from
Skilling, the estimates of $Z$ are unbiased.

%Throughout this paper we use the popular `overloaded' notation for some
%functions and probability distributions.
%For example, $L(\xx)$ is the likelihood function, and $L(X)$ is the likelihood
%as a function of the enclosed prior mass $X$. The inverse, $X(L)$, takes a
%likelihood as input and computes the corresponding enclosed prior mass.
%Therefore the symbol $L$ implies the output of the function is a
%likelihood value; so $L(\xx)$ represents a likelihood value computed from
%the parameters $\xx$, and $L(X)$ represents a likelihood value that
%corresponds to an amount $X$ of prior mass. In more traditional notation,
%$X(L)$ would be written using function composition, $X(L(\xx))$.

\section{Accessing distributions other than the posterior}
Using a single NS run, we can also calculate the properties of any
distribution that
is (in some sense which we do not make precise) {\em intermediate}
between the prior and the posterior. For example, we might be interested in
a `power posterior' where the likelihood is raised to a power $\beta$
(this is also the family of distributions used in `annealing' or `tempering'
methods):
\begin{eqnarray}
p(\xx; \beta) &=& \frac{\pi(\xx)L(\xx)^\beta}
                       {Z(\beta)}\label{eqn:power_posterior}
\end{eqnarray}
The normalisation and posterior samples from this distribution can be obtained
from the original NS run by re-weighting the output according to $L(\xx)^\beta$
instead of the usual $L(\xx)$ used to obtain the posterior.
Bayesian inference problems with `gaussian noise' likelihood assumptions
provide an example of where power posteriors might be useful.
Computing $p(\xx; \beta)$ for $\beta \neq 1$ allows us to explore what the
posterior distribution would have been if the noise variance had been greater,
without having to re-run the algorithm.
This is different from including an extra parameter to allow the noise variance
to be greater, because NS allows you to test the consequences of values of the
variance that are very implausible given the data, i.e. that do not have
a high posterior probability.

Alternatives to NS include methods based on `annealing', where a sequence of
distributions of the form of Equation~\ref{eqn:power_posterior} is used.
There are many different methods based on this idea, such as the popular
parallel tempering \citep{hansmann1997parallel, gregory2005bayesian},
simulated tempering \citep{marinari1992simulated},
annealed importance sampling \citep{neal2001annealed}, and
steppingstone sampling \citep{xie2010improving}.
However, unlike annealing methods, NS requires only a small number of
tuning parameters (just the number of particles $N$ is inherent to NS, although
most NS implementations have additional tuning parameters related to their
methods for sampling Equation~\ref{eq:constrained_prior}). Annealing methods
also tend to fail on phase change problems \citep{skilling} which can arise
in data analysis \citep{rjobject, exoplanet} as well as in statistical
mechanics, where they are more familiar.

In statistical mechanics, Equation~\ref{eqn:power_posterior} defines the
family of {\it canonical distributions}, usually written as:
\begin{eqnarray}
p(\xx; \beta) &=& \frac{\pi(\xx)\exp[-\beta E(\xx)]}{Z(\beta)}
\end{eqnarray}
where $E(\xx)$ is the energy function (analogous to minus the log likelihood
in the Bayesian case), and $\pi(\xx)$ is often uniform over
phase space (the set of possible positions and momenta of a collection of
particles) or configuration space (the set of possible positions of a collection
of particles).

In this context we usually want to
compute $Z(\beta)$ as a function of $\beta$, which is called the
{\it partition function}. When the full phase space of positions and momenta
is included, the KL-divergence as a function of temperature,
$H(\beta)$, is the Gibbs entropy (up to a sign change and a constant factor).
A single run of NS can be used to reconstruct $Z(\beta)$ and $H(\beta)$,
allowing the study of
the properties of materials from first principles, based on hypotheses about
the atoms or molecules that make them up
\citep[e.g.][]{partay2010efficient, baldock2016determining}.
The NS exploration algorithm is invariant under
monotonic transformations of $L$ or $E$ and therefore there we can discuss the
algorithm in Bayesian or statistical mechanical terms without loss of
generality.

\subsection{Notation for Objective Functions}
Throughout this paper we will refer to $L(\xx)$ or
$-E(\xx)$ as `objective functions',
by analogy with the terminology used in optimization.
Although we are interested in sampling rather
than optimization, we still want to increase the values of $L$ or decrease
the values of $E$ relative to what is typical of the prior --- just in
a controlled manner, so we can quantify prior mass as we ascend.

Throughout this paper and the software, there are three
conventions used to define the objective functions. $L()$ is named by analogy
with likelihood, and appears multiplicatively as in
Equation~\ref{eqn:power_posterior}. $E() = -\ln L()$ is named by analogy with
`energy'. Finally, in the software,
$S() = \ln L() = -E()$ is often used (the $S$ stands for a
`{\em S}calar function of $\xx$'). The algorithm ascends $L$ and $S$, but
descends $E$.

\section{Multiple objective functions}
In some inference and
statistical mechanics problems, there are two or more scalar functions of
$\xx$ that are relevant. Suppose our prior is $\pi(\xx)$ as before, and
we obtain information that fixes the expected values of two scalar
``likelihood'' functions of $\xx$, $L_1(\xx)$ and $L_2(\xx)$
(or equivalently, ``energy'' functions $E_1(\xx) = -\ln L_1(\xx)$ and
$E_2 = -\ln L_2(\xx)$). The
updated probability distribution that takes into account the constraints is
of the canonical form \citep{jaynes1957information}:
\begin{align}
p(\xx; \beta_1, \beta_2) &=
    \frac{\pi(\xx)L_1(\xx)^{\beta_1}L_2(\xx)^{\beta_2}}
         {Z(\beta_1, \beta_2)} \\
    &=
    \frac{\pi(\xx)\exp\left[-\beta_1E_1(\xx) - \beta_2E_2(\xx)\right]}
         {Z(\beta_1, \beta_2)}
\end{align}
where $\beta_1$ and $\beta_2$ are two `inverse temperatures' (the temperatures
themselves are $T_1 = \beta_1^{-1}$ and $T_2 = \beta_2^{-1}$).
In a Bayesian
context, this distribution (when $\beta_1 = \beta_2 = 1$) could be the
posterior distribution for parameters $\xx$
given two datasets, where $L_1$ and $L_2$ are the
likelihood functions for each dataset. In statistical mechanics, $E_1$ might
be the total energy and $E_2$ might be the total
angular momentum, or perhaps a term depending on an external field (so $\beta_2$
would then describe the strength of the external field).

If we are only interested in a single canonical distribution, for example
with $\beta_1 = 0.3$ and $\beta_2 = 0.7$, we can estimate its normalising
constant and by running standard Nested Sampling with objective function
$L(\xx) = L_1(\xx)^{\beta_1}L_2(\xx)^{\beta_2}$. However, what if we
are interested in a range of values for $\beta_1$ and $\beta_2$, and we
want to know the entire partition function $Z(\beta_1, \beta_2)$?
This work describes
progress towards solving this class of problems while maintaining the benefits
of Nested Sampling, such as the ability to cope with phase
transitions. The algorithm is presented here for problems with two objective
functions, but the implementation allows for more than two (though the
performance is those cases is likely to be poor).


\section{Algorithm requirements}
The prior $\pi(\xx)$ implies a certain prior for $L_1$ and $L_2$, which we
denote $\pi(L_1, L_2)$, overloading the $\pi$ symbol.
An example of a prior is shown as the
probability density in Figure~\ref{fig:joint1}. The partition function
$Z(\beta_1, \beta_2)$ is a set of expected values with respect to this density.
By analogy with the standard case of one objective function,
simple Monte Carlo sampling from $\pi$ will not work except for values of
$(\beta_1, \beta_2)$ where the canonical distributions of interest are
not too different from $\pi$.
We need a sampler that explores regions where $L_1$ and $L_2$ are increased
(relative to what's typical of $\pi$) in order
to accurately estimate $Z(\beta_1, \beta_2)$. The specific way of increasing
$L_1$ and $L_2$ should follow the requirements listed in the following
subsection, which are unique to NS. Another unique property of NS which our
algorithm {\em does not} satisfy is discussed in Appendix~\ref{sec:property}.

%\begin{figure}[ht!]
%\centering
%\includegraphics[scale=0.5]{figures/joint1.pdf}
%\caption{\it An example of a prior for $L_1$ and $L_2$, which is implied by
%the prior $\pi(\xx)$. We have overplotted 25 points drawn from this prior.
%The estimated prior mass in the shaded rectangle is
%$\hat{X}(2, 1.5) = 5/25 = 0.2$.
%\label{fig:joint1}}
%\end{figure}

%Our proposed algorithm (introduced in Section~\ref{sec:algorithm}) makes use
%of the (two dimensional) cumulative distribution of the objective functions.
%For any values $(\ell_1, \ell_2)$ of the objective functions,
%the prior mass for which both $L_1 \geq \ell_1$ {\em and} $L_2 \geq \ell_2$ is:
%\begin{eqnarray}
%X(\ell_1, \ell_2) &=& \int \pi(L_1, L_2)
%\mathds{1}\left[L_1 \geq \ell_1, L_2 \geq \ell_2 \right]
% \, dL_1 \, dL_2.
%\end{eqnarray}
%Since $X$ is a probability, it must satisfy a product rule:
%\begin{eqnarray}
%X(\ell_1, \ell_2) = X(\ell_1)X(\ell_2 | \ell_1) = X(\ell_2)X(\ell_1 | \ell_2).
%\end{eqnarray}

%When the prior $\pi$ is approximated with a set of $N$
%particles $\{\xx_i\}_{i=1}^N$
%(Monte Carlo samples ``drawn from'' $\pi$),
%$X(\ell_1, \ell_2)$ can be approximated by:
%\begin{eqnarray}
%\hat{X}(\ell_1, \ell_2) &=&
%\frac{1}{N}
%\sum_{i=1}^N \mathds{1}\left[L_1(\xx_i) \geq \ell_1,
%L_2(\xx_i) \geq \ell_2\right].\label{eqn:corner_count}
%\end{eqnarray}
%Graphically, on a plot of $L_2$ vs. $L_1$, $\hat{X}(\ell_1, \ell_2)$
%is the fraction of particles in the rectangle whose upper right corner is at
%$\left(\ell_1, \ell_2\right)$. See Figure~\ref{fig:joint1} for an
%example.

\subsection{Desiderata}
While we could not see how to satisfy the property in
Appendix~\ref{sec:property},
the algorithm of this paper does satisfy the following other NS properties:
\begin{enumerate}
\item It begins with $N$ points drawn from the prior $\pi(\xx)$.
\item It seeks to explore regions where the values of
$L_1(\xx)$ and $L_2(\xx)$ are higher than the prior $\pi(L_1, L_2)$
would typically imply.
\item It considers a sequence of probability
distributions proportional to $\pi$, but restricted to smaller and smaller
domains for which the enclosed prior mass shrinks by a (roughly) constant and
known factor at each iteration.
\item The exploration procedure is invariant under monotonic transformations of
$L_1$ and $L_2$, i.e. it should only depend on rankings of $L_1$ and $L_2$
values and not on the numeric values themselves.
\end{enumerate}
The standard Nested Sampling algorithm has these properties but for a
single objective function. The same is true of variants such as
Diffusive Nested Sampling \citep{dns, dnest4} and PolyChord
\citep{handley2015polychord}.

\subsection{The algorithm}
The algorithm itself is rather simple. It consists of a collection of
{\em reps} (short for repetitions) of Nested Sampling, each of which ascends
a different compromise between $L_1$ and $L_2$. The output of the reps can then
be used, with appropriate weights, to represent canonical distributions at
any temperature.

The combination of the reps makes use of the (approximate?) unbiasedness
property of Nested Sampling when the prior weights of the dead particles are
assigned proportional to
$[(N-1)/N]^i$ at iteration $i$, instead of being based on
Skilling's geometric sequence of $X$-values given by $\exp(-i/N)$.
\citet{walter2017point} demonstrated that the Nested Sampling estimator of $Z$
is unbiased with these weights, but assuming perfect sampling of the replacement
particles at each iteration. Additionally, using the framework of
Sequential Monte Carlo \citep{doucet2001introduction},
\citet{salomone2018unbiased} showed that this remains true even if imperfect
MCMC is used to generate new particles. Technically, this property is
lost if the sequence of distributions is chosen adaptively (a key feature of NS)
rather than being fixed in advanced. However, this paper demonstrates that the
algorithm works well in practice despite this limitation.
{\todo Verify/prove statement about bias, and that it means I can average
reps the way I'm doing it. Also check this remains the case with the random
thinning that I'm using.}

The fact that $Z$ estimates can be made unbiased means that separate $Z$
estimates can be averaged together and the properties of the averaged estimator
will be easily understood and satisfactory ({\todo probably need to be more
technical here.}). {\todo The following statement is based on intuition}
Runs that ascend the `wrong' objective function are still unbiased
estimates of $Z$ for the `right' objective function. Some of the reps will
be well-positioned to represent some canonical distributions but inappropriate
for others. When this happens, the particles from the rep will just have very
low weights with respect to some canonical distributions.

{\todo A description of the algorithm for a single rep follows}.
For each rep, we set up a probability distribution over the objective functions,
given by probabilities $\boldsymbol{p} = \{p_1, ..., p_n\}$,
where $n$ is the number of
objective functions. If there
are only two objective functions, there is just a probability $p_1$ for
objective function 1, and $p_2 = 1-p_1$ for objective function 2.

The first distribution is the prior $\pi(\xx)$. Objective function thresholds
$(\ell_1, \ell_2)_0 = (-\infty, -\infty)$ are assigned.
$N$ particles are generated
from the prior. Then, an objective function is selected according to the
probability distribution $\boldsymbol{p}$.
Suppose the objective function chosen is number $k$.
Then the worst particle wrt that criteron is written to disk and replaced,
and the threshold is updated in the $k$th position only.

Essentially, a `rep' is just Nested Sampling, but where a threshold is
maintained for all objective functions. At each iteration, the space is
restricted by the value of only one of the objective functions.

The probability distribution $\boldsymbol{p}$ influences the
proportion of the compressions that are done using each objective function.
Different reps ought to have different values for the probability distribution.
Some reps will mostly increase $L_1$, some will mostly increase $L_2$, and
some will be in between. In the reference implementation
(Appendix~\ref{sec:software}), the log-probabilities are chosen from a $t$
distribution with 2 degrees of freedom, exponentiated, and then normalized.




\begin{algorithm}[!ht]
    \setstretch{1.35}
    Set $N$, the number of particles \\
    Set $M$, the number of reps \\
    Set $d$, the target depth in nats \\
    Compute $J = Nd$, the number of NS iterations per rep \\
    \For{i in 1:$M$}{
      Set initial threshold $(\ell_1^*, \ell_2^*) = (0, 0)$ \\
      Generate initial particles $(\xx_1, ..., \xx_N)$ from prior $\pi$ \\
      Generate a `direction' (probability distribution over objective functions) $\boldsymbol{p}$ \\
      \For{j in 1:$J$}{
          Choose an objective function $k$ according to $\boldsymbol{p}$ \\
          Find $c = \textnormal{argmin}(\lambda C \to L_k(\xx_C))$, the index of the worst particle \\
          Write out $\xx_c$ to disk as $\xx_{ij}$ with prior weight $w_{ij}$ proportional to $(1 - 1/N)^j$ \\
          Replace threshold element $\ell_c^*$ with $L_k(\xx_c)$ \\
          Replace particle $c$ with new particle generated from
            $\pi\times\mathds{1}(L_1 > \ell_1^*, L_2 > \ell_2^*)$          
      }
    }

    Set $(T_1, T_2)$, the temperatures of a canonical distribution of interest \\
    Compute $\hat{Z}(T_1, T_2) \leftarrow \sum_{ij} w_{ij}\left(L_1(\xx_{ij})/T_1 + L_2(\xx_{ij})/T_2\right) / \sum_{ij} w_{ij}$\\
  \vspace*{0.5em}
  \caption{The `Switch Sampling' algorithm.}
\end{algorithm}


\section{Example 1: Known solution}
In the software repository, this example is implemented in
{\tt examples/Demo.h} and {\tt examples/Demo.cpp}.
We tested the algorithm on a simple 100-dimensional example, where the
prior is a uniform distribution between 0 and 1 for each coordinate:
\begin{eqnarray}
\pi(\xx) &=& \prod_{i=1}^{100}
\left\{
\begin{array}{lr}
1, & x_i \in [0, 1]\\
0, & \textnormal{otherwise.}
\end{array}
\right.
\end{eqnarray}
The first objective function is quadratic in form, such that the canonical
distributions are iid normal centered at $(0.5, 0.5, ..., 0.5)$:
\begin{eqnarray}
E_1(\xx) &=& \sum_{i=1}^{100} \left(\frac{x_i - 0.5}{0.1}\right)^2.
\end{eqnarray}
The second objective function, given by
\begin{eqnarray}
E_2(\xx) &=& \sum_{i=1}^{100}\sin^2(10 \pi x_i),
\end{eqnarray}
upweights states where coordinates $\{x_i\}$
are close to 0, 0.1, 0.2, ..., 0.9 and 1.
Since both of the objective functions are simple sums over the coordinates,
the canonical distributions are products of independent distributions for
each coordinate.

The canonical distributions for this system should draw the coordinates
towards 0.5 (due to $E_1$) and towards the troughs of $E_2$, with the
relative importance of these effects determined by the values of the
inverse temperatures $\beta_1$ and $\beta_2$. Three example canonical
distributions are shown in Figure~\ref{fig:demo_canonical}.
The true partition function (computed numerically) and the Kullback-Leibler
divergence from the prior to the canonical distribution are shown in
Figure~\ref{fig:demo_truth} as a function of two temperatures. These show the
common behaviour where lower temperatures correspond to lower values
of the normalising constant $\log Z$ and higher values of the KL divergence
$H$. There are no phase changes, which would result in discontinuities.


\begin{figure}[!ht]
\centering
\includegraphics[scale=0.6]{figures/demo_canonical.pdf}
\caption{Three canonical distributions for different values of the two
temperatures $T_1$ and $T_2$, in the simple demo problem.
These are for the one-dimensional case, but
in this example, the 100 dimensional canonical distributions are `independent
and identically distributed' version of this distribution.
Low values of $T_1$ bring
more probability towards the centre of the domain, and low values of $T_2$
bring out the oscillatory structure in the probability density function.
\label{fig:demo_canonical}}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.6]{figures/demo_truth.pdf}
\caption{The true partition function and KL divergence (from the prior to
the canonical distribution) as a function of two temperatures for the
test problem.\label{fig:demo_truth}}
\end{figure}


The algorithm was run for 1000 reps, to a `depth' of 500 nats,
with 100 particles and 1000 MCMC steps per NS iteration per rep.
Results are given in
Figures~\ref{fig:demo_logZ_H},~\ref{fig:demo_residuals},
and~\ref{fig:demo_N_eff}.


\begin{figure}[!ht]
\centering
\includegraphics[scale=0.6]{figures/demo_logZ_H.pdf}
\caption{The partition function $\ln(Z)$ and information $H$ as a function
of the two temperatures, reconstructed using the algorithm. Compare with
Figure~\ref{fig:demo_truth}.
\label{fig:demo_logZ_H}}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.6]{figures/demo_residuals.pdf}
\caption{The residuals, i.e., the difference between
Figures~\ref{fig:demo_logZ_H} and~\ref{fig:demo_truth}.
\label{fig:demo_residuals}}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[scale=0.6]{figures/demo_N_eff.pdf}
\caption{The error in $\ln(Z)$, expressed in terms of an
`effective number of particles' in a standard NS run that would lead to
the same error at 1 sigma. No values are below $N_{\rm eff} = 1$.
\label{fig:demo_N_eff}}
\end{figure}

%\begin{figure}[ht!]
%\centering
%\includegraphics[scale=1]{figures/output.png}
%\caption{The sequence of discarded particles (yellow) and
%points representing the canonical distribution at temperatures
%$(T_1, T_2) = (0.1, 1)$ (black).\label{fig:output}}
%\end{figure}



%\begin{figure}[ht!]
%\centering
%\includegraphics[scale=0.5]{figures/results.pdf}
%\caption{Numerical results for the test problem. These compare well with the
%true partition function and KL divergence given in Figure~\ref{fig:truth}.
%\label{fig:results}}
%\end{figure}

\section{Example 2: Lennard-Jones/Polymer Example}

Consider $n$ Lennard-Jones atoms confined to the unit cube $[0, 1]^3$.
The position of each atom is given by a vector
$\boldsymbol{r}_i = (x_i, y_i, z_i)$.
The potential energy of any configuration of the atoms is given by
a sum over all pairs of atoms:
\begin{eqnarray}
U_{\rm LJ} &=& \sum_{i=1}^n\sum_{j=(i+1)}^n U_{\rm LJ}
(\boldsymbol{r}_i, \boldsymbol{r}_j)
\end{eqnarray}
where the inter-atomic potential energy is given by:

\begin{eqnarray}
U_{\rm LJ}(\boldsymbol{r}_i, \boldsymbol{r}_j)
&=& 4\epsilon
\left[
\left(\frac{\sigma}{|\boldsymbol{r}_i - \boldsymbol{r}_j|}\right)^12 -
\left(\frac{\sigma}{|\boldsymbol{r}_i - \boldsymbol{r}_j|}\right)^6
\right].
\end{eqnarray}
We adopted the Lennard Jones units by setting $\epsilon=1$ and $\sigma=1$.

In terms of the Lennard Jones parameters $\epsilon$ and $\sigma$, the
``polymer'' potential is
\begin{eqnarray}
U_{\rm polymer} &=&
\sum_{i=1}^{n-1} \phi^{(1)}_{i,i+1} + \sum_{i=2}^{n-1} \phi^{(2)}_{i-1, i, i+1}
\end{eqnarray}
where

\begin{eqnarray}
\phi^{(1)}_{i,i+1} &=& \frac{k}{2}\left(|\boldsymbol{r}_i - \boldsymbol{r}_{i+1}|- 2^{1/6}\sigma\right)^2,\\
\phi^{(2)}_{i-1, i, i+1} &=& \frac{k}{2}
\left((\boldsymbol{r}_i - \boldsymbol{r}_{i-1})\cdot(\boldsymbol{r}_{i-1} - \boldsymbol{r}_{i}) - 1\right)^2,
\end{eqnarray}
and $k=\frac{36 \times 2^{2/3}\epsilon}{\sigma^2}$ has been chosen to match
the curvature of $U_{\rm LJ}(\boldsymbol{r}_i, \boldsymbol{r}_j)$.


%\section{Example 3: ``MaxEnt'' image deconvolution example}

%Problems with more than one objective function can also arise in data
%analysis. Consider, for example, the problem of deblurring a noisy
%image. A common approach to these problems is regularized inversion,
%such as the ``maximum entropy'' technique of \citet{gull}. It is important
%to distinguish between this method and the maximum entropy principle used
%to update or assign probabilities \citep{caticha}.

%The unknown parameters are a set of pixel intensities
%$\{x_i\}_{i=1}^n$ where $n$ is the number of pixels and $x_i \geq 0$.
%The dataset is also a collection of pixel intensities $\{D_i\}_{i=1}^n$.
%The prior for the dataset given the parameters is a normal distribution
%with mean given by the underlying image $\{xi\}$ convolved by
%a point spread function. The standard deviation of each pixel is
%$\sigma$, assumed known. This is usually written as

%\begin{eqnarray}
%\boldsymbol{D} = \boldsymbol{M}\boldsymbol{x} + \boldsymbol{n}
%\end{eqnarray}
%where $\boldsymbol{n}$ is the noise in each pixel and $\boldsymbol{M}$ is
%the blurring operator written in matrix form.

%The prior for the pixel intensities $\{x_i\}$ has an ``entropic'' form,
%where images are upweighted (relative to a uniform prior) if the pixel
%intensities have a low dynamic range. The prior density is defined conditional
%on a hyperparameter $\lambda$:
%\begin{eqnarray}
%p(\boldsymbol{x} | \lambda) &=&
%\frac{\exp\left[\lambda S(\xx)\right]}{\int \exp\left[\lambda S(\xx)\right] \, d\xx}
%\label{eq:entropy_prior}
%\end{eqnarray}
%where
%\begin{eqnarray}
%S(\xx) &=& -\sum_{i=1}^n x_i \ln x_i
%\end{eqnarray}
%is the ``configurational entropy'' of the image (crucially, this is a function of
%an image, and not a function of a probability distribution).
%The denominator in Equation~\ref{eq:entropy_prior} ensures the conditional prior
%is normalized.

%In the 1980s, this prior was thought to have a fundamental status, but this
%is no longer the case \citep{massinf}, although the function $S(\xx)$ does
%have special properties when its argument is a measure \citep{knuth_skilling}.
%However, it is still a prior whose
%consequences we might be interested in.

%Instead, we can consider the configurational
%entropy $S(\xx)$ as simply a function of an image that may
%be relevant somehow, and that we are uncertain about. The joint posterior
%for $\xx$ and $\lambda$ is:
%\begin{eqnarray}
%p(\lambda, \xx | \data) &=&
%\frac{p(\lambda)p(\xx | \lambda)p(\data | \xx)}
%{p(\data)}\\
%&=&
%\frac{p(\lambda)p(\xx | \lambda)p(\data | \xx)}
%{p(\data)}
%\end{eqnarray}


%The posterior for the underlying image given the data is
%\begin{eqnarray}
%p(\boldsymbol{x} | \boldsymbol{D}, \lambda)
%&=& \frac{p(\boldsymbol{x} | \lambda)p(\boldsymbol{D}|\boldsymbol{x}, \lambda)}
%{\int p(\boldsymbol{x}|\lambda)p(\boldsymbol{D}|\boldsymbol{x}, \lambda) \,d^n\boldsymbol{m}}\\
%&=& \frac{\exp\left[-\lambda \sum_i x_i\ln x_i
%-\frac{1}{2\sigma^2}\left(D_i - \sum_{j} P_{ij}x_{j}\right)^2\right]}
%{\int \exp\left[-\lambda \sum_i x_i\ln x_i
%-\frac{1}{2\sigma^2}\left(D_i - \sum_{j} P_{ij}x_{j}\right)^2\right]\, d^n\boldsymbol{m}}
%\end{eqnarray}

%This is a `doubly intractable' problem \citep{murray}, since an integral, whose
%result is a function of the parameters, appears in the {\em denominator} of the
%target density. However, in our case the integral in the
%denominator arises from the prior for the parameters, rather than from the
%likelihood function.

%The introduction should briefly place the study in a broad context and highlight why it is important. It should define the purpose of the work and its significance. The current state of the research field should be reviewed carefully and key publications cited. Please highlight controversial and diverging hypotheses when necessary. Finally, briefly mention the main aim of the work and highlight the principal conclusions. As far as possible, please keep the introduction comprehensible to scientists outside your particular field of research. Citing a journal paper \cite{ref-journal}. And now citing a book reference \cite{ref-book}. Please use the command \citep{ref-journal} for the following MDPI journals, which use author-date citation: Administrative Sciences, Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, JRFM, Languages, Laws, Religions, Risks, Social Sciences.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Results}

%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
%\begin{quote}
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
%\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Subsection}

%\subsubsection{Subsubsection}

%Bulleted lists look like this:
%\begin{itemize}[leftmargin=*,labelsep=5.8mm]
%\item	First bullet
%\item	Second bullet
%\item	Third bullet
%\end{itemize}

%Numbered lists can be added as follows:
%\begin{enumerate}[leftmargin=*,labelsep=4.9mm]
%\item	First item 
%\item	Second item
%\item	Third item
%\end{enumerate}

%The text continues here.

%\subsection{Figures, Tables and Schemes}

%All figures and tables should be cited in the main text as Figure 1, Table 1, etc.

%\begin{figure}[H]
%\centering
%\includegraphics[width=2 cm]{Definitions/logo-mdpi}
%\caption{This is a figure, Schemes follow the same formatting. If there are multiple panels, they should be listed as: (\textbf{a}) Description of what is contained in the first panel. (\textbf{b}) Description of what is contained in the second panel. Figures should be placed in the main text near to the first time they are cited. A caption on a single line should be centered.}
%\end{figure}   

%\begin{table}[H]
%\caption{This is a table caption. Tables should be placed in the main text near to the first time they are cited.}
%\centering
%%% \tablesize{} %% You can specify the fontsize here, e.g.  \tablesize{\footnotesize}. If commented out \small will be used.
%\begin{tabular}{ccc}
%\toprule
%\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
%\midrule
%entry 1		& data			& data\\
%entry 2		& data			& data\\
%\bottomrule
%\end{tabular}
%\end{table}

%\subsection{Formatting of Mathematical Components}

%This is an example of an equation:

%\begin{equation}
%a + b = c
%\end{equation}
%%% If the documentclass option "submit" is chosen, please insert a blank line before and after any math environment (equation and eqnarray environments). This ensures correct linenumbering. The blank line should be removed when the documentclass option is changed to "accept" because the text following an equation should not be a new paragraph. 

%Please punctuate equations as regular text. Theorem-type environments (including propositions, lemmas, corollaries etc.) can be formatted as follows:
%%% Example of a theorem:
%\begin{Theorem}
%Example text of a theorem.
%\end{Theorem}

%The text continues here. Proofs must be formatted as follows:

%%% Example of a proof:
%\begin{proof}[Proof of Theorem 1]
%Text of the proof. Note that the phrase `of Theorem 1' is optional if it is clear which theorem is being referred to.
%\end{proof}
%The text continues here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Discussion}

%Authors should discuss the results and how they can be interpreted in perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Materials and Methods}

%Materials and Methods should be described with sufficient details to allow others to replicate and build on published results. Please note that publication of your manuscript implicates that you must make all materials, data, computer code, and protocols associated with the publication available to readers. Please disclose at the submission stage any restrictions on the availability of materials or information. New methods and protocols should be described in detail while well-established methods can be briefly described and appropriately cited.

%Research manuscripts reporting large datasets that are deposited in a publicly available database should specify where the data have been deposited and provide the relevant accession numbers. If the accession numbers have not yet been obtained at the time of submission, please state that they will be provided during review. They must be provided prior to publication.

%Interventionary studies involving animals or humans, and other studies require ethical approval must list the authority that provided approval and the corresponding ethical approval code. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

%This section is not mandatory, but can be added to the manuscript if the discussion is unusually long or complex.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Patents}
%This section is not mandatory, but may be added if there are patents resulting from the work reported in this manuscript.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{BJB conceived the problem and worked on various ideas
over the years that nearly worked. RJNB suggested and worked on the polymer
example.
EC discussed the problem and worked on
several attempts at solutions with BJB over the years.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\funding{This research received no external funding.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{BJB acknowledges research and study leave funding from the
University of Auckland. We would like to thank Gábor Csányi,
Livia Bartók-Pártay,
Rob Salomone, John Skilling, etc etc.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictsofinterest{The authors declare no conflict of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\abbreviations{The following abbreviations are used in this manuscript:\\

\noindent 
\begin{tabular}{@{}ll}
MCMC & Markov Chain Monte Carlo \\
NS & Nested Sampling \\
SMC & Sequential Monte Carlo
\end{tabular}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\appendixtitles{yes} %Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixsections{multiple} %Leave argument "multiple" if there are multiple sections. Then a counter is printed ("Appendix A"). If there is only one appendix section then change the argument to "one" and no counter is printed ("Appendix").
\appendix
\section{A Unique Property of Nested Sampling}\label{sec:property}
What is special about the NS sequence of distributions (the constrained priors)?
Intuitively, the fact they are chosen adaptively (i.e., the likelihood values
observed
at one iteration determine the likelihood threshold for the next constrained
distribution)
provides a practical advantage
over annealing-based methods (and reduces the number of tuning parameters
substantially --- no `annealing schedule' is needed). While adaptive methods
for generating annealing schedules exist \citep{salomone2018unbiased}, these
are of no use in some phase change problems where a single temperature implies
a mixture of two or more phases.

Another feature of NS is that its output can be used to reconstruct
any constrained prior. Consider the Nested Sampling measure, proportional
to the amount of time
NS spends in each region of the space (see \citet{polson2014vertical}
for further discussion of this concept, and a derivation of it from some
desiderata).
Since NS moves at a constant rate
in terms of $\log(X)$, this measure is proportional to
\begin{align}
m_{\rm NS} &\propto \frac{\pi(\xx)}{X(L(\xx))}.\label{eqn:measure}
\end{align}
and is improper if we imagine an NS run that does not terminate.
Consider using $m_{NS}$ as an importance sampling-type distribution to
estimate properties of a constrained prior
$p(\xx ; \ell) = \pi(\xx)\mathds{1}(L(\xx) > \ell)/X(\ell)$.
How useful $m_{\rm NS}$ is for determining properties of $p(\xx ; \ell)$
may be indicated by the KL-divergence from the former to the latter:
\begin{align}
D_{\rm KL}\left(p(\xx ; \ell) || m_{\rm NS}(\xx)\right)
    &= \int p(\xx ; \ell)
                \log \left(\frac{p(\xx ; \ell)}{m_{\rm NS}(\xx)}\right)
                \, d\xx \\
    &= \textnormal{const} + \int_{L(\xx) > \ell} \frac{\pi(\xx)}{X(\ell)}
                \log \left(\frac{X(L(\xx))}{X(\ell)}\right) \, d\xx
\end{align}
Rewriting the integral to be with respect to $X$,
and taking advantage of the
fact the implied prior for $X$ is uniform (since $X$ is the complementary
CDF of the likelihood values under $\pi$), gives:
\begin{align}
D_{\rm KL}\left(p(\xx ; \ell) || m_{\rm NS}(\xx)\right)
    &= \textnormal{const} + \frac{1}{X(\ell)}\int_0^{X(\ell)} 
                \log \left(\frac{X}{X(\ell)}\right) \, dX\\
    &= \textnormal{const} + \frac{1}{X(\ell)}\left(
                    X(\ell)\log \left(\frac{X(\ell)}{X(\ell)}\right) - X(\ell)\right)\\
    &= \textnormal{const} -1 = \textnormal{const}'.
\end{align}

Remarkably, this does not depend on $\ell$. This means that every constrained
prior, no matter the value of its threshold $\ell$, is equally `distant'
from $m_{\rm NS}$. This can be seen intuitively using a standard
$\log L$-$\log X$ plot from NS. On these plots, the prior is a reflected
exponential distribution with scale 1, and the likelihood constraints
merely truncate the exponential distribution (by the memorylessness property
of the exponential, truncation is equivalent to shifting). The NS measure
is uniform in terms of $\log(X)$, so we have essentially shown that the KL
divergence from a wide uniform measure to a narrow shifted
exponential (of unit width) contained within it does not depend on the
position of the exponential.

Unfortunately, this property breaks down with more than
one objective function, and (as far as we know) the algorithm given in this
paper does not satisfy an analogous property.

\section{Using the TwinPeaks2018 software}\label{sec:software}



%\section{}
%All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: internal bibliography
%=====================================
\reftitle{References}

%\bibliographystyle{mdpi}
\bibliography{references.bib/references}

% The following MDPI journals use author-date citation: Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, JRFM, Laws, Religions, Risks, Social Sciences. For those journals, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%=====================================
% References, variant B: external bibliography
%=====================================
%\externalbibliography{yes}
%\bibliography{your_external_BibTeX_file}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\sampleavailability{Samples of the compounds ...... are available from the authors.}

%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

